{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MNIST via Keras\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\miniconda3\\envs\\tf-1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Program\\miniconda3\\envs\\tf-1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Program\\miniconda3\\envs\\tf-1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Program\\miniconda3\\envs\\tf-1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Program\\miniconda3\\envs\\tf-1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Program\\miniconda3\\envs\\tf-1.12\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version %s\" % tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run the cell below if \"failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\" error happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "# config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.2651 - acc: 0.9191 - val_loss: 0.0580 - val_acc: 0.9817\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.0873 - acc: 0.9733 - val_loss: 0.0448 - val_acc: 0.9849\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.0661 - acc: 0.9799 - val_loss: 0.0378 - val_acc: 0.9875\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0543 - acc: 0.9839 - val_loss: 0.0314 - val_acc: 0.9897\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0475 - acc: 0.9860 - val_loss: 0.0316 - val_acc: 0.9891\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0416 - acc: 0.9878 - val_loss: 0.0281 - val_acc: 0.9905\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0383 - acc: 0.9880 - val_loss: 0.0297 - val_acc: 0.9906\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0352 - acc: 0.9894 - val_loss: 0.0289 - val_acc: 0.9911\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0318 - acc: 0.9901 - val_loss: 0.0277 - val_acc: 0.9903\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0304 - acc: 0.9905 - val_loss: 0.0317 - val_acc: 0.9900\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0280 - acc: 0.9914 - val_loss: 0.0269 - val_acc: 0.9918\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0277 - acc: 0.9910 - val_loss: 0.0289 - val_acc: 0.9911\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0264 - acc: 0.9920 - val_loss: 0.0301 - val_acc: 0.9908\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0264 - acc: 0.9921 - val_loss: 0.0273 - val_acc: 0.9919\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0242 - acc: 0.9928 - val_loss: 0.0293 - val_acc: 0.9918\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0218 - acc: 0.9930 - val_loss: 0.0321 - val_acc: 0.9908\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0223 - acc: 0.9930 - val_loss: 0.0275 - val_acc: 0.9920\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0210 - acc: 0.9933 - val_loss: 0.0270 - val_acc: 0.9915\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0207 - acc: 0.9934 - val_loss: 0.0273 - val_acc: 0.9915\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0186 - acc: 0.9944 - val_loss: 0.0312 - val_acc: 0.9918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2972b657128>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.03118970596088875\n",
      "Test accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
